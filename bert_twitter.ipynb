{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (4.48.2)\n",
      "Requirement already satisfied: datasets in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dmitrij/Документы/envs/base1/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as tf \n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Union, List\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google-bert/bert-base-uncased'\n",
    "DATA_NAME = 'SetFit/emotion'\n",
    "CHECKPOINTS = 'checkpoints_twitter'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TEST_SIZE = 0.1\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 3\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0, 'label_text': 'sadness'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(set(data['test']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "itoe = {}\n",
    "for sample in data['train']:\n",
    "    if sample['label'] not in itoe.keys():\n",
    "        itoe[sample['label']] = sample['label_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'sadness', 3: 'anger', 2: 'love', 5: 'surprise', 4: 'fear', 1: 'joy'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClassifierModel(nn.Module):\n",
    "    def __init__(self, backbone: Union[str, nn.Module],\n",
    "                 embed_shape: int = 768,\n",
    "                 num_cls: int = NUM_CLASSES):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(backbone, nn.Module):\n",
    "            self.backbone = backbone\n",
    "        else:\n",
    "            self.backbone = tf.AutoModel.from_pretrained(backbone)\n",
    "\n",
    "        self.classifier = nn.Linear(embed_shape, num_cls)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        att = (x != 0).to(torch.int)\n",
    "\n",
    "        out = self.backbone(input_ids=x, attention_mask=att).pooler_output\n",
    "        logits = self.classifier(out)\n",
    "        return logits\n",
    "        \n",
    "model = TwitterClassifierModel(MODEL_NAME).to(DEVICE)\n",
    "tokenizer = tf.AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0130, -0.7420, -0.1304,  0.1563, -0.3831,  0.2253]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'hello word!'\n",
    "tokenized_text = tokenizer(text, padding=True, truncation=True).input_ids\n",
    "model(torch.LongTensor(tokenized_text).unsqueeze(0).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(sample):\n",
    "    return tokenizer(sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sample['text']  for sample in data['train']]\n",
    "X_test = [sample['text'] for sample in data['test']]\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train, padding=True).input_ids\n",
    "X_test_tokenized = tokenizer(X_test, padding=True).input_ids\n",
    "\n",
    "train_dataset = [(torch.tensor(X_train_tokenized[i]), torch.tensor(data['train'][i]['label'])) for i in range(len(X_train))]\n",
    "test_dataset = [(torch.tensor(X_test_tokenized[i]), torch.tensor(data['test'][i]['label'])) for i in range(len(X_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                            #   collate_fn=collate_fn_with_padding\n",
    "                              )\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=True,\n",
    "                            #  collate_fn=collate_fn_with_padding\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_backbone_fn(model: TwitterClassifierModel):\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwitterClassifierModel(\n",
       "  (backbone): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "    return (y_pred == y_true).to(torch.int16).reshape(-1).sum(0) / y_true.size(0) / y_true.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(transformer_model: nn.Module, \n",
    "                      freeze_backbone: bool = True, \n",
    "                      lr: int = LR,\n",
    "                      epochs: int = EPOCHS):\n",
    "    model = copy.deepcopy(transformer_model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    if freeze_backbone:\n",
    "        model = freeze_backbone_fn(model)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        loss_sum = 0\n",
    "        acc_sum = 0\n",
    "        print(f'------------------epoch: {i + 1}------------------')\n",
    "        for X, y in tqdm(train_dataloader):\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            acc = accuracy(y.unsqueeze(1), y_pred.argmax(-1).unsqueeze(1))\n",
    "            loss_sum += loss\n",
    "            acc_sum += acc\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'train loss {loss_sum / len(list(train_dataloader))} acc {acc_sum / len(list(train_dataloader))}')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformer(transformer_model: nn.Module):\n",
    "    model = copy.deepcopy(transformer_model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "\n",
    "    model.eval()\n",
    "    for X, y in tqdm(test_dataloader):\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.inference_mode():\n",
    "            y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        acc = accuracy(y.unsqueeze(1), y_pred.argmax(-1).unsqueeze(1))\n",
    "        loss_sum += loss\n",
    "        acc_sum += acc\n",
    "    print(f'test loss {loss_sum / len(list(test_dataloader))} acc {acc_sum / len(list(test_dataloader))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------epoch: 1------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [05:08<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.6073542833328247 acc 0.7920041680335999\n",
      "------------------epoch: 2------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [05:10<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.1512235701084137 acc 0.937631368637085\n",
      "------------------epoch: 3------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [05:11<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.10863865911960602 acc 0.9497554302215576\n"
     ]
    }
   ],
   "source": [
    "model = train_transformer(model, freeze_backbone=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:08<00:00, 24.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.17035098373889923 acc 0.9249998331069946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_transformer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text: Union[str, List[str]], model: nn.Module = model):\n",
    "    model.eval()\n",
    "\n",
    "    tokenized_text = tokenizer(text, padding=True, truncation=True).input_ids\n",
    "    logits = model(torch.LongTensor(tokenized_text).unsqueeze(0).to(DEVICE))\n",
    "\n",
    "    return itoe[logits.argmax(-1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference('I feel very bad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
